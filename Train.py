# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18MJJbvpheWBoaJuHqknk3h6ADRI6dLYe
"""

import torch
import torch.nn as nn
import torchvision
from torchvision import transforms
import torch.utils.data as data
from tqdm import tqdm
import wandb
import argparse

wandb.init(project="transferlearning_persion_mnist")

my_parser = argparse.ArgumentParser()
my_parser.add_argument('--device',default='cpu', type=str)
args=my_parser.parse_args()

device = torch.device('cuda' if torch.cuda.is_available() and args.device=='GPU' else 'cpu')
model = torchvision.models.resnet50(pretrained=True)

# Transfer Learning
in_features = model.fc.in_features
model.fc = nn.Linear(in_features, 10)

model=model.to(device)

# config = wandb.config
learning_rate = 0.025
batch_size = 2
epochs = 30

transform =transforms.Compose([
                                transforms.Resize((64, 64)),
                                transforms.ToTensor(),
                                transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))
])

dataset = torchvision.datasets.ImageFolder(root='/content/drive/MyDrive/MNIST_persian/MNIST_persian', transform=transform)
torch.manual_seed(0)
train_size = int(len(dataset)*0.8)
val_size = len(dataset)-train_size
train_data ,val_data = data.random_split(dataset,[train_size,val_size])

train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,num_workers=1)
val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False,num_workers=1)

# compile
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
loss_function = nn.CrossEntropyLoss()

def calc_acc(preds, labels):
    preds_max = torch.argmax(preds, 1)
    acc = torch.sum(preds_max == labels.data, dtype=torch.float16) / len(preds)
    return acc

def train(model, train_data_loader,epoch):
  wandb.watch(model)
  model.train(True)
  train_loss=0.0
  train_acc=0.0
  for images,labels in tqdm(train_data_loader):
    images=images.to(device)
    labels=labels.to(device)
    optimizer.zero_grad()
    
    preds_train = model(images)

    loss_train=loss_function(preds_train,labels) # loss_train
    loss_train.backward()

    optimizer.step()

    train_loss += loss_train
    train_acc += calc_acc(preds_train,labels)
    
  total_loss = train_loss/len(train_data_loader)
  total_acc = train_acc/len(train_data_loader)

  if epoch % 2 == 0:
    wandb.log({"epoch": epoch})
    wandb.log({"loss_train": total_loss})
    wandb.log({"acc_train": total_acc})

  print(f"loss_train:{total_loss},accuracy_train:{total_acc}")

for epoch in range(epochs):
  print(f'Epoch:{epoch}')
  train(model, train_data_loader, epoch)

torch.save(model.state_dict(), "persion_mnist.pth")